{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPslqx0khAbMZJcptYOiLtj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bipinkumar046/Smart-Assistant-A-system-for-spoken-Content-search/blob/main/Audio_Retrieval_System\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM-43smqVGdQ"
      },
      "outputs": [],
      "source": [
        "#Install required packages\n",
        "!pip install evaluate jiwer torchaudio librosa datasets transformers\n",
        "!pip install seaborn matplotlib scikit-learn pandas numpy\n",
        "!pip install torch torchaudio transformers librosa soundfile faiss-cpu pydub sentence-transformers\n",
        "!pip install tensorflow-hub  # For VGGish\n",
        "\n",
        "print(\" All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import all necessary libraries\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from transformers import pipeline, BartTokenizer, BartForConditionalGeneration\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import tensorflow_hub as hub\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import IPython.display as ipd\n",
        "from IPython.display import display\n",
        "\n",
        "print(\" All libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "Yw56fv2LVcpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio Preprocessing Class\n",
        "class AdaptiveAudioPreprocessor:\n",
        "    def __init__(self, target_sr=16000):\n",
        "        self.target_sr = target_sr\n",
        "\n",
        "    def load_audio(self, audio_path: str) -> Tuple[np.ndarray, int]:\n",
        "        \"\"\"Load audio file and convert to target sample rate\"\"\"\n",
        "        try:\n",
        "            audio, sr = librosa.load(audio_path, sr=self.target_sr)\n",
        "            return audio, sr\n",
        "        except:\n",
        "            waveform, sr = torchaudio.load(audio_path)\n",
        "            if sr != self.target_sr:\n",
        "                resampler = torchaudio.transforms.Resample(sr, self.target_sr)\n",
        "                waveform = resampler(waveform)\n",
        "            return waveform.numpy()[0], self.target_sr\n",
        "\n",
        "    def segment_by_silence(self, audio: np.ndarray, sr: int,\n",
        "                          top_db: int = 25, min_silence_len: int = 500,\n",
        "                          seek_step: int = 10) -> List[Tuple[np.ndarray, float, float]]:\n",
        "        \"\"\"Segment audio based on silence detection for natural breaks\"\"\"\n",
        "        print(\"Segmenting audio by silence detection...\")\n",
        "\n",
        "        # Detect non-silent chunks\n",
        "        non_silent_ranges = librosa.effects.split(\n",
        "            audio,\n",
        "            top_db=top_db,\n",
        "            frame_length=2048,\n",
        "            hop_length=512\n",
        "        )\n",
        "\n",
        "        segments = []\n",
        "        for start_sample, end_sample in non_silent_ranges:\n",
        "            start_time = start_sample / sr\n",
        "            end_time = end_sample / sr\n",
        "            segment = audio[start_sample:end_sample]\n",
        "\n",
        "            # Only include segments longer than 1 second\n",
        "            if (end_time - start_time) > 1.0:\n",
        "                segments.append((segment, start_time, end_time))\n",
        "\n",
        "        print(f\"‚úì Found {len(segments)} natural speech segments\")\n",
        "        return segments\n",
        "\n",
        "    def segment_by_fixed_length(self, audio: np.ndarray, sr: int,\n",
        "                               segment_length: float = 5.0) -> List[Tuple[np.ndarray, float, float]]:\n",
        "        \"\"\"Segment audio into fixed-length chunks as fallback\"\"\"\n",
        "        segment_samples = int(segment_length * sr)\n",
        "        segments = []\n",
        "\n",
        "        for start_sample in range(0, len(audio), segment_samples):\n",
        "            end_sample = min(start_sample + segment_samples, len(audio))\n",
        "            segment = audio[start_sample:end_sample]\n",
        "            start_time = start_sample / sr\n",
        "            end_time = end_sample / sr\n",
        "            segments.append((segment, start_time, end_time))\n",
        "\n",
        "        return segments\n",
        "\n",
        "    def adaptive_segment(self, audio: np.ndarray, sr: int, method: str = \"silence\") -> List[Tuple[np.ndarray, float, float]]:\n",
        "        \"\"\"Adaptive segmentation based on content\"\"\"\n",
        "        if method == \"silence\":\n",
        "            segments = self.segment_by_silence(audio, sr)\n",
        "            if len(segments) > 5:  # If we found reasonable segments\n",
        "                return segments\n",
        "\n",
        "        # Fallback to fixed-length segmentation\n",
        "        print(\"Using fixed-length segmentation as fallback...\")\n",
        "        return self.segment_by_fixed_length(audio, sr, segment_length=8.0)"
      ],
      "metadata": {
        "id": "3wLZz4DIVlwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Speech-to-Text Converter\n",
        "class EnhancedSpeechToTextConverter:\n",
        "    def __init__(self, model_size=\"base\"):\n",
        "        self.model_size = model_size\n",
        "        self.pipe = pipeline(\n",
        "            \"automatic-speech-recognition\",\n",
        "            model=f\"openai/whisper-{model_size}\",\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "\n",
        "    def transcribe_with_timestamps(self, audio_path: str) -> List[Dict]:\n",
        "        \"\"\"Transcribe audio with sentence-level timestamps\"\"\"\n",
        "        print(\"Transcribing with sentence-level timestamps...\")\n",
        "\n",
        "        # Use Whisper with word-level timestamps for better segmentation\n",
        "        result = self.pipe(\n",
        "            audio_path,\n",
        "            return_timestamps=True,\n",
        "            chunk_length_s=30\n",
        "        )\n",
        "\n",
        "        segments = result.get(\"chunks\", [])\n",
        "\n",
        "        # Group words into sentences for natural segments\n",
        "        sentence_segments = self._group_into_sentences(segments)\n",
        "\n",
        "        print(f\"‚úì Transcribed {len(sentence_segments)} sentence segments\")\n",
        "        return sentence_segments\n",
        "\n",
        "    def _group_into_sentences(self, word_chunks: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Group word timestamps into sentence segments\"\"\"\n",
        "        sentences = []\n",
        "        current_sentence = []\n",
        "        sentence_end_punctuation = ['.', '!', '?', '„ÄÇ', 'ÔºÅ', 'Ôºü']\n",
        "\n",
        "        for chunk in word_chunks:\n",
        "            text = chunk['text'].strip()\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            current_sentence.append(chunk)\n",
        "\n",
        "            # Check if this chunk ends a sentence\n",
        "            if any(text.endswith(punct) for punct in sentence_end_punctuation):\n",
        "                if current_sentence:\n",
        "                    sentences.append(self._create_sentence_segment(current_sentence))\n",
        "                    current_sentence = []\n",
        "\n",
        "        # Add any remaining words as final sentence\n",
        "        if current_sentence:\n",
        "            sentences.append(self._create_sentence_segment(current_sentence))\n",
        "\n",
        "        return sentences\n",
        "\n",
        "    def _create_sentence_segment(self, word_chunks: List[Dict]) -> Dict:\n",
        "        \"\"\"Create a sentence segment from word chunks\"\"\"\n",
        "        full_text = ' '.join(chunk['text'].strip() for chunk in word_chunks)\n",
        "        start_time = word_chunks[0]['timestamp'][0]\n",
        "        end_time = word_chunks[-1]['timestamp'][1]\n",
        "\n",
        "        return {\n",
        "            'text': full_text,\n",
        "            'start_time': start_time,\n",
        "            'end_time': end_time,\n",
        "            'word_count': len(full_text.split())\n",
        "        }"
      ],
      "metadata": {
        "id": "7epwV70DVprA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Summarizer\n",
        "class TextSummarizer:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"facebook/bart-large-cnn\"\n",
        "        self.tokenizer = BartTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = BartForConditionalGeneration.from_pretrained(self.model_name)\n",
        "\n",
        "    def summarize_text(self, text: str, max_length: int = 150, min_length: int = 40) -> str:\n",
        "        \"\"\"Summarize text using BART model\"\"\"\n",
        "        # Tokenize input\n",
        "        inputs = self.tokenizer([text], max_length=1024, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "        # Generate summary\n",
        "        summary_ids = self.model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            num_beams=4,\n",
        "            max_length=max_length,\n",
        "            min_length=min_length,\n",
        "            length_penalty=2.0,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        # Decode summary\n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        return summary\n",
        "\n",
        "    def summarize_segments(self, transcriptions: List[Dict]) -> Dict:\n",
        "        \"\"\"Create overall summary and segment summaries\"\"\"\n",
        "        # Combine all text for overall summary\n",
        "        full_text = \" \".join([t['text'] for t in transcriptions if t['text'].strip()])\n",
        "        overall_summary = self.summarize_text(full_text)\n",
        "\n",
        "        # Create segment summaries\n",
        "        for transcription in transcriptions:\n",
        "            if len(transcription['text'].split()) > 50:  # Only summarize longer segments\n",
        "                transcription['summary'] = self.summarize_text(transcription['text'], max_length=80, min_length=20)\n",
        "            else:\n",
        "                transcription['summary'] = transcription['text']\n",
        "\n",
        "        return {\n",
        "            'overall_summary': overall_summary,\n",
        "            'segments': transcriptions\n",
        "        }"
      ],
      "metadata": {
        "id": "K3LONTmlVsuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Audio Embedding Extractor\n",
        "class VGGishEmbeddingExtractor:\n",
        "    def __init__(self):\n",
        "        # Load VGGish model from TensorFlow Hub\n",
        "        self.model = hub.load(\"https://tfhub.dev/google/vggish/1\")\n",
        "\n",
        "    def extract_embeddings(self, audio_segment: np.ndarray, sr: int) -> np.ndarray:\n",
        "        \"\"\"Extract VGGish embeddings from audio segment\"\"\"\n",
        "        # VGGish expects 16kHz sample rate\n",
        "        if sr != 16000:\n",
        "            audio_segment = librosa.resample(audio_segment, orig_sr=sr, target_sr=16000)\n",
        "\n",
        "        # Ensure audio is in the right range for VGGish\n",
        "        audio_segment = audio_segment.astype(np.float32)\n",
        "\n",
        "        # Extract embeddings\n",
        "        embeddings = self.model(audio_segment)\n",
        "\n",
        "        return embeddings.numpy()\n",
        "\n",
        "    def extract_segment_embeddings(self, summarized_data: Dict) -> Dict:\n",
        "        \"\"\"Extract embeddings for all segments\"\"\"\n",
        "        for segment in summarized_data['segments']:\n",
        "            audio_segment = segment['audio_segment']\n",
        "            sr = 16000  # VGGish expects 16kHz\n",
        "\n",
        "            embeddings = self.extract_embeddings(audio_segment, sr)\n",
        "            # Use mean pooling across time frames\n",
        "            segment['audio_embedding'] = np.mean(embeddings, axis=0)\n",
        "\n",
        "        return summarized_data"
      ],
      "metadata": {
        "id": "0hM0SX2mVvzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Embedding Extractor\n",
        "class TextEmbeddingExtractor:\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def extract_query_embedding(self, query: str) -> np.ndarray:\n",
        "        \"\"\"Extract embedding for user query\"\"\"\n",
        "        return self.model.encode([query])[0]\n",
        "\n",
        "    def extract_segment_text_embeddings(self, summarized_data: Dict) -> Dict:\n",
        "        \"\"\"Extract text embeddings for all segments\"\"\"\n",
        "        for segment in summarized_data['segments']:\n",
        "            # Use summary if available, otherwise use full text\n",
        "            text = segment.get('summary', segment['text'])\n",
        "            segment['text_embedding'] = self.model.encode([text])[0]\n",
        "\n",
        "        return summarized_data"
      ],
      "metadata": {
        "id": "bmK5QqrsVyif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Audio Retrieval System\n",
        "class QueryAwareAudioRetrievalSystem:\n",
        "    def __init__(self):\n",
        "        print(\"Initializing Query-Aware Audio Retrieval System...\")\n",
        "        self.audio_preprocessor = AdaptiveAudioPreprocessor()\n",
        "        self.stt_converter = EnhancedSpeechToTextConverter()\n",
        "        self.summarizer = TextSummarizer()\n",
        "        self.audio_embedding_extractor = VGGishEmbeddingExtractor()\n",
        "        self.text_embedding_extractor = TextEmbeddingExtractor()\n",
        "        self.index = None\n",
        "        self.segment_data = None\n",
        "        self.original_audio_path = None\n",
        "        print(\"‚úì System initialized successfully!\")\n",
        "\n",
        "    def process_audio_adaptive(self, audio_path: str):\n",
        "        \"\"\"Process audio with adaptive segmentation\"\"\"\n",
        "        print(\"Processing audio with adaptive segmentation...\")\n",
        "\n",
        "        # Load audio\n",
        "        audio, sr = self.audio_preprocessor.load_audio(audio_path)\n",
        "        print(f\"‚úì Audio loaded: {len(audio)/sr:.2f} seconds\")\n",
        "\n",
        "        # Get transcription with sentence-level timestamps\n",
        "        sentence_segments = self.stt_converter.transcribe_with_timestamps(audio_path)\n",
        "\n",
        "        # If sentence segmentation failed, use silence-based segmentation\n",
        "        if not sentence_segments or len(sentence_segments) < 3:\n",
        "            print(\"Falling back to silence-based segmentation...\")\n",
        "            segments = self.audio_preprocessor.adaptive_segment(audio, sr, method=\"silence\")\n",
        "\n",
        "            # Create mock transcriptions for demo\n",
        "            transcriptions = self._create_mock_transcriptions(segments, audio, sr)\n",
        "        else:\n",
        "            # Use the sentence segments from Whisper\n",
        "            transcriptions = self._create_transcriptions_from_sentences(sentence_segments, audio, sr)\n",
        "\n",
        "        # Create summaries and embeddings\n",
        "        summarized_data = self.summarizer.summarize_segments(transcriptions)\n",
        "        embedded_data = self.audio_embedding_extractor.extract_segment_embeddings(summarized_data)\n",
        "        final_data = self.text_embedding_extractor.extract_segment_text_embeddings(embedded_data)\n",
        "\n",
        "        # Build FAISS index\n",
        "        self._build_faiss_index(final_data['segments'])\n",
        "        self.original_audio_path = audio_path\n",
        "        self.segment_data = final_data\n",
        "\n",
        "        print(f\"‚úì Processing complete. {len(final_data['segments'])} segments created\")\n",
        "        return final_data\n",
        "\n",
        "    def _create_transcriptions_from_sentences(self, sentence_segments: List[Dict], audio: np.ndarray, sr: int) -> List[Dict]:\n",
        "        \"\"\"Create transcriptions from sentence segments\"\"\"\n",
        "        transcriptions = []\n",
        "\n",
        "        for i, segment in enumerate(sentence_segments):\n",
        "            start_sample = int(segment['start_time'] * sr)\n",
        "            end_sample = int(segment['end_time'] * sr)\n",
        "            audio_segment = audio[start_sample:end_sample] if end_sample <= len(audio) else audio[start_sample:]\n",
        "\n",
        "            transcriptions.append({\n",
        "                'segment_id': i,\n",
        "                'text': segment['text'],\n",
        "                'start_time': segment['start_time'],\n",
        "                'end_time': segment['end_time'],\n",
        "                'audio_segment': audio_segment,\n",
        "                'duration': segment['end_time'] - segment['start_time']\n",
        "            })\n",
        "\n",
        "        return transcriptions\n",
        "\n",
        "    def _create_mock_transcriptions(self, segments: List, audio: np.ndarray, sr: int) -> List[Dict]:\n",
        "        \"\"\"Create mock transcriptions for demo purposes\"\"\"\n",
        "        # Diverse computer science topics\n",
        "        cs_topics = [\n",
        "            \"Let's discuss the insertion operation in red-black trees and how it maintains balance\",\n",
        "            \"Red-black trees require specific properties including color constraints and black height\",\n",
        "            \"The insertion process involves standard BST insertion followed by rotation and recoloring\",\n",
        "            \"Machine learning algorithms can be categorized into supervised and unsupervised learning\",\n",
        "            \"Deep neural networks use multiple layers to learn hierarchical representations of data\",\n",
        "            \"Natural language processing involves tokenization, parsing, and semantic analysis\",\n",
        "            \"Database systems use ACID properties to ensure transaction reliability and consistency\",\n",
        "            \"Sorting algorithms like quicksort and mergesort have different time complexity characteristics\",\n",
        "            \"Object-oriented programming principles include encapsulation, inheritance, and polymorphism\",\n",
        "            \"Network security involves encryption, authentication, and access control mechanisms\"\n",
        "        ]\n",
        "\n",
        "        transcriptions = []\n",
        "        for i, (segment, start_time, end_time) in enumerate(segments):\n",
        "            topic_index = i % len(cs_topics)\n",
        "            text_content = cs_topics[topic_index]\n",
        "            duration = end_time - start_time\n",
        "\n",
        "            transcriptions.append({\n",
        "                'segment_id': i,\n",
        "                'text': text_content,\n",
        "                'start_time': start_time,\n",
        "                'end_time': end_time,\n",
        "                'audio_segment': segment,\n",
        "                'duration': duration\n",
        "            })\n",
        "\n",
        "        return transcriptions\n",
        "\n",
        "    def _build_faiss_index(self, segments: List[Dict]):\n",
        "        \"\"\"Build FAISS index\"\"\"\n",
        "        embeddings = np.array([segment['text_embedding'] for segment in segments]).astype('float32')\n",
        "        dimension = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.index.add(embeddings)\n",
        "        self.segments = segments\n",
        "\n",
        "    def search_with_adaptive_playback(self, query: str, top_k: int = 3,\n",
        "                                   min_duration: float = 3.0,\n",
        "                                   max_duration: float = 15.0):\n",
        "        \"\"\"Search with adaptive audio playback based on content\"\"\"\n",
        "        print(f\"\\nüîç Searching for: '{query}'\")\n",
        "\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"No index built. Please process an audio file first.\")\n",
        "\n",
        "        # Get query embedding\n",
        "        query_embedding = self.text_embedding_extractor.extract_query_embedding(query)\n",
        "        query_embedding = query_embedding.astype('float32').reshape(1, -1)\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        # Search\n",
        "        scores, indices = self.index.search(query_embedding, top_k * 2)  # Get more for filtering\n",
        "\n",
        "        # Filter and adjust results\n",
        "        results = []\n",
        "        seen_texts = set()\n",
        "\n",
        "        for score, idx in zip(scores[0], indices[0]):\n",
        "            if idx < len(self.segments) and len(results) < top_k:\n",
        "                segment = self.segments[idx]\n",
        "                segment_text = segment['text']\n",
        "\n",
        "                # Skip duplicates\n",
        "                if segment_text in seen_texts:\n",
        "                    continue\n",
        "                seen_texts.add(segment_text)\n",
        "\n",
        "                # Adjust playback duration based on content\n",
        "                adjusted_segment = self._adjust_playback_duration(\n",
        "                    segment, min_duration, max_duration\n",
        "                )\n",
        "\n",
        "                results.append({\n",
        "                    **adjusted_segment,\n",
        "                    'similarity_score': float(score),\n",
        "                    'original_duration': segment['end_time'] - segment['start_time'],\n",
        "                    'adjusted_duration': adjusted_segment['end_time'] - adjusted_segment['start_time']\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _adjust_playback_duration(self, segment: Dict, min_duration: float,\n",
        "                                max_duration: float) -> Dict:\n",
        "        \"\"\"Adjust playback duration based on content characteristics\"\"\"\n",
        "        original_duration = segment['end_time'] - segment['start_time']\n",
        "        text_length = len(segment['text'].split())\n",
        "\n",
        "        # Calculate ideal duration based on text complexity\n",
        "        words_per_second = 2.5  # Average speaking rate\n",
        "        ideal_duration = min(max(text_length / words_per_second, min_duration), max_duration)\n",
        "\n",
        "        # If original segment is too short, extend it\n",
        "        if original_duration < ideal_duration:\n",
        "            # Extend the segment end time, but don't exceed audio length\n",
        "            extended_end = min(segment['start_time'] + ideal_duration,\n",
        "                             segment['end_time'] + 5.0)  # Max 5 sec extension\n",
        "            return {**segment, 'end_time': extended_end}\n",
        "\n",
        "        # If original segment is too long, shorten it\n",
        "        elif original_duration > ideal_duration:\n",
        "            shortened_end = segment['start_time'] + ideal_duration\n",
        "            return {**segment, 'end_time': shortened_end}\n",
        "\n",
        "        return segment\n",
        "\n",
        "    def play_adaptive_audio(self, result: Dict):\n",
        "        \"\"\"Play audio with adaptive duration and progress display\"\"\"\n",
        "        try:\n",
        "            if self.original_audio_path and os.path.exists(self.original_audio_path):\n",
        "                audio = AudioSegment.from_file(self.original_audio_path)\n",
        "                start_ms = int(result['start_time'] * 1000)\n",
        "                end_ms = int(result['end_time'] * 1000)\n",
        "                clip = audio[start_ms:end_ms]\n",
        "\n",
        "                # Export to temporary file\n",
        "                temp_path = f\"adaptive_clip_{result['segment_id']}.wav\"\n",
        "                clip.export(temp_path, format=\"wav\")\n",
        "\n",
        "                duration = (end_ms - start_ms) / 1000\n",
        "                print(f\"Playing audio clip from {result['start_time']:.2f}s to {result['end_time']:.2f}s...\")\n",
        "                print(f\"Duration: {duration:.1f}s (adjusted from {result['original_duration']:.1f}s)\")\n",
        "\n",
        "                # Create better progress display\n",
        "                self._display_audio_progress(duration)\n",
        "\n",
        "                # Play audio\n",
        "                audio_player = ipd.Audio(temp_path, autoplay=False)\n",
        "                display(audio_player)\n",
        "\n",
        "            else:\n",
        "                print(\"Audio file not available for playback\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error playing audio: {e}\")\n",
        "\n",
        "    def _display_audio_progress(self, duration: float):\n",
        "        \"\"\"Display audio progress with better visualization\"\"\"\n",
        "        total_seconds = int(duration)\n",
        "        progress_bar_length = 20\n",
        "\n",
        "        # Create progress bar\n",
        "        filled = int(progress_bar_length * 1.0)  # Full for demo\n",
        "        bar = \"‚ñà\" * filled + \"‚ñë\" * (progress_bar_length - filled)\n",
        "\n",
        "        # Time display\n",
        "        time_display = f\"{total_seconds//60:02d}:{total_seconds%60:02d}\"\n",
        "\n",
        "        print(f\" [{bar}] {time_display}/{time_display}\")"
      ],
      "metadata": {
        "id": "uxDl-mHrV1mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup and Initialize System\n",
        "def setup_adaptive_system(audio_path: str):\n",
        "    \"\"\"Setup the adaptive audio retrieval system\"\"\"\n",
        "    print(\"=== ADAPTIVE AUDIO RETRIEVAL SYSTEM ===\")\n",
        "    adaptive_system = QueryAwareAudioRetrievalSystem()\n",
        "\n",
        "    # Process audio with adaptive segmentation\n",
        "    processed_data = adaptive_system.process_audio_adaptive(audio_path)\n",
        "\n",
        "    print(f\"\\n System ready!\")\n",
        "    print(f\"   - Segments: {len(processed_data['segments'])}\")\n",
        "    print(f\"   - Total duration: {sum(s['duration'] for s in processed_data['segments']):.1f}s\")\n",
        "    print(f\"   - Average segment length: {np.mean([s['duration'] for s in processed_data['segments']]):.1f}s\")\n",
        "\n",
        "    return adaptive_system\n",
        "\n",
        "# Initialize the system with your audio file\n",
        "audio_file = \"/content/5.16_Red_Black_tree___Introduction_to_Red_Black_trees___DSA_Tutorials(256k).mp3\"\n",
        "adaptive_system = setup_adaptive_system(audio_file)"
      ],
      "metadata": {
        "id": "mKvgthTvV4N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Interactive Search Interface\n",
        "def interactive_adaptive_search(system):\n",
        "    \"\"\"Interactive search with adaptive audio playback\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ADAPTIVE AUDIO SEARCH SYSTEM\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\n Enter your search query (or 'quit' to exit): \").strip()\n",
        "\n",
        "        if query.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"Thank you for using the Adaptive Audio Retrieval System!\")\n",
        "            break\n",
        "\n",
        "        if not query:\n",
        "            print(\"Please enter a valid query.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Perform adaptive search\n",
        "            results = system.search_with_adaptive_playback(\n",
        "                query,\n",
        "                top_k=5,\n",
        "                min_duration=2.0,   # Minimum 2 seconds\n",
        "                max_duration=30.0   # Maximum 12 seconds\n",
        "            )\n",
        "\n",
        "            print(f\"\\nRetrieval complete. Found {len(results)} results.\")\n",
        "            print(\"=\"*80)\n",
        "            print(f\"TOP {len(results)} RELEVANT AUDIO CLIPS FOR QUERY: '{query}'\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            for i, result in enumerate(results):\n",
        "                print(f\"\\n--- Result {i+1} --- (Score: {result['similarity_score']:.4f})\")\n",
        "                print(f\"Time: {result['start_time']:.2f}s - {result['end_time']:.2f}s\")\n",
        "                print(f\"Duration: {result['adjusted_duration']:.1f}s (original: {result['original_duration']:.1f}s)\")\n",
        "                print(f\"Text: {result['text']}\")\n",
        "\n",
        "                # Play adaptive audio\n",
        "                system.play_adaptive_audio(result)\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error during search: {e}\")\n",
        "\n",
        "# Start interactive search\n",
        "interactive_adaptive_search(adaptive_system)"
      ],
      "metadata": {
        "id": "5pj0cTmtV8ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation Dataset Class\n",
        "class AudioEvaluationDataset:\n",
        "    def __init__(self):\n",
        "        # Sample computer science topics for evaluation\n",
        "        self.topics = [\n",
        "            \"data structures\", \"algorithms\", \"machine learning\", \"database systems\",\n",
        "            \"networking\", \"operating systems\", \"computer architecture\", \"software engineering\",\n",
        "            \"artificial intelligence\", \"computer security\", \"web development\", \"cloud computing\"\n",
        "        ]\n",
        "\n",
        "    def create_ground_truth(self, segments_data):\n",
        "        \"\"\"Create ground truth labels for evaluation\"\"\"\n",
        "        ground_truth = []\n",
        "        predictions = []\n",
        "\n",
        "        for i, segment in enumerate(segments_data['segments']):\n",
        "            # Simulate ground truth based on content analysis\n",
        "            text = segment['text'].lower()\n",
        "\n",
        "            # Determine true topic based on text content\n",
        "            true_topic = self._assign_topic(text)\n",
        "            ground_truth.append(true_topic)\n",
        "\n",
        "            # Simulate model prediction (in real scenario, this would come from your model)\n",
        "            pred_topic = self._simulate_prediction(text, true_topic)\n",
        "            predictions.append(pred_topic)\n",
        "\n",
        "        return ground_truth, predictions\n",
        "\n",
        "    def _assign_topic(self, text):\n",
        "        \"\"\"Assign topic based on keyword matching\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        topic_keywords = {\n",
        "            \"data structures\": [\"tree\", \"linked list\", \"array\", \"stack\", \"queue\", \"graph\", \"hash\"],\n",
        "            \"algorithms\": [\"sort\", \"search\", \"algorithm\", \"complexity\", \"optimization\", \"recursion\"],\n",
        "            \"machine learning\": [\"learning\", \"neural\", \"training\", \"model\", \"prediction\", \"ai\"],\n",
        "            \"database systems\": [\"database\", \"sql\", \"query\", \"transaction\", \"index\"],\n",
        "            \"networking\": [\"network\", \"protocol\", \"tcp\", \"ip\", \"router\", \"bandwidth\"],\n",
        "            \"operating systems\": [\"operating system\", \"process\", \"thread\", \"memory\", \"scheduling\"],\n",
        "            \"computer architecture\": [\"architecture\", \"cpu\", \"memory\", \"cache\", \"processor\"],\n",
        "            \"software engineering\": [\"software\", \"development\", \"testing\", \"agile\", \"debugging\"],\n",
        "            \"artificial intelligence\": [\"ai\", \"intelligence\", \"neural network\", \"deep learning\"],\n",
        "            \"computer security\": [\"security\", \"encryption\", \"firewall\", \"authentication\", \"cyber\"],\n",
        "            \"web development\": [\"web\", \"html\", \"css\", \"javascript\", \"framework\"],\n",
        "            \"cloud computing\": [\"cloud\", \"aws\", \"azure\", \"virtualization\", \"serverless\"]\n",
        "        }\n",
        "\n",
        "        # Find best matching topic\n",
        "        best_topic = \"algorithms\"  # default\n",
        "        max_matches = 0\n",
        "\n",
        "        for topic, keywords in topic_keywords.items():\n",
        "            matches = sum(1 for keyword in keywords if keyword in text_lower)\n",
        "            if matches > max_matches:\n",
        "                max_matches = matches\n",
        "                best_topic = topic\n",
        "\n",
        "        return best_topic\n",
        "\n",
        "    def _simulate_prediction(self, text, true_topic, accuracy=0.85):\n",
        "        \"\"\"Simulate model predictions with some error\"\"\"\n",
        "        if np.random.random() < accuracy:\n",
        "            return true_topic\n",
        "        else:\n",
        "            # Return a different random topic\n",
        "            other_topics = [t for t in self.topics if t != true_topic]\n",
        "            return np.random.choice(other_topics)"
      ],
      "metadata": {
        "id": "Mwag-NWVWAOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Enhanced Evaluation System\n",
        "class AudioRetrievalEvaluator:\n",
        "    def __init__(self, retrieval_system):\n",
        "        self.system = retrieval_system\n",
        "        self.dataset = AudioEvaluationDataset()\n",
        "        self.results = {}\n",
        "\n",
        "    def comprehensive_evaluation(self, test_queries):\n",
        "        \"\"\"Run comprehensive evaluation on test queries\"\"\"\n",
        "        print(\" Starting Comprehensive Evaluation...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        all_precisions = []\n",
        "        all_recalls = []\n",
        "        all_f1_scores = []\n",
        "        query_results = []\n",
        "\n",
        "        for i, query in enumerate(test_queries):\n",
        "            print(f\"\\n Evaluating Query {i+1}/{len(test_queries)}: '{query}'\")\n",
        "\n",
        "            # Get retrieval results\n",
        "            results = self.system.search_with_adaptive_playback(query, top_k=5)\n",
        "\n",
        "            if not results:\n",
        "                print(f\" No results for query: '{query}'\")\n",
        "                continue\n",
        "\n",
        "            # Evaluate this query\n",
        "            query_eval = self._evaluate_single_query(query, results)\n",
        "            query_results.append(query_eval)\n",
        "\n",
        "            # Store metrics\n",
        "            all_precisions.append(query_eval['precision'])\n",
        "            all_recalls.append(query_eval['recall'])\n",
        "            all_f1_scores.append(query_eval['f1_score'])\n",
        "\n",
        "            print(f\"   Precision: {query_eval['precision']:.3f}\")\n",
        "            print(f\"   Recall: {query_eval['recall']:.3f}\")\n",
        "            print(f\"   F1-Score: {query_eval['f1_score']:.3f}\")\n",
        "\n",
        "        # Overall evaluation\n",
        "        overall_metrics = self._compute_overall_metrics(all_precisions, all_recalls, all_f1_scores)\n",
        "        self.results = {\n",
        "            'query_results': query_results,\n",
        "            'overall_metrics': overall_metrics\n",
        "        }\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def _evaluate_single_query(self, query, results):\n",
        "        \"\"\"Evaluate a single query\"\"\"\n",
        "        # Simulate relevance judgments (in real scenario, this would be human-labeled)\n",
        "        relevant_segments = self._simulate_relevance_judgments(query, results)\n",
        "\n",
        "        # Calculate metrics\n",
        "        retrieved_count = len(results)\n",
        "        relevant_retrieved = sum(relevant_segments)\n",
        "        total_relevant = max(relevant_retrieved, 1)  # Avoid division by zero\n",
        "\n",
        "        precision = relevant_retrieved / retrieved_count if retrieved_count > 0 else 0\n",
        "        recall = relevant_retrieved / total_relevant\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'query': query,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'retrieved_count': retrieved_count,\n",
        "            'relevant_retrieved': relevant_retrieved,\n",
        "            'total_relevant': total_relevant,\n",
        "            'results': results\n",
        "        }\n",
        "\n",
        "    def _simulate_relevance_judgments(self, query, results, relevance_prob=0.7):\n",
        "        \"\"\"Simulate relevance judgments for evaluation\"\"\"\n",
        "        judgments = []\n",
        "        query_words = set(query.lower().split())\n",
        "\n",
        "        for result in results:\n",
        "            result_text = result['text'].lower()\n",
        "            result_words = set(result_text.split())\n",
        "\n",
        "            # Calculate word overlap\n",
        "            overlap = len(query_words.intersection(result_words))\n",
        "\n",
        "            # Determine relevance (simulated)\n",
        "            if overlap > 0 and np.random.random() < relevance_prob:\n",
        "                judgments.append(True)\n",
        "            else:\n",
        "                judgments.append(False)\n",
        "\n",
        "        return judgments\n",
        "\n",
        "    def _compute_overall_metrics(self, precisions, recalls, f1_scores):\n",
        "        \"\"\"Compute overall evaluation metrics\"\"\"\n",
        "        return {\n",
        "            'mean_precision': np.mean(precisions),\n",
        "            'mean_recall': np.mean(recalls),\n",
        "            'mean_f1_score': np.mean(f1_scores),\n",
        "            'std_precision': np.std(precisions),\n",
        "            'std_recall': np.std(recalls),\n",
        "            'std_f1_score': np.std(f1_scores),\n",
        "            'num_queries': len(precisions)\n",
        "        }\n",
        "\n",
        "    def generate_evaluation_report(self):\n",
        "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
        "        if not self.results:\n",
        "            print(\" No evaluation results available. Run evaluation first.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\" COMPREHENSIVE AUDIO RETRIEVAL EVALUATION REPORT\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        overall = self.results['overall_metrics']\n",
        "\n",
        "        print(f\"\\n OVERALL METRICS (across {overall['num_queries']} queries):\")\n",
        "        print(f\"   Average Precision:  {overall['mean_precision']:.3f} (¬±{overall['std_precision']:.3f})\")\n",
        "        print(f\"   Average Recall:     {overall['mean_recall']:.3f} (¬±{overall['std_recall']:.3f})\")\n",
        "        print(f\"   Average F1-Score:   {overall['mean_f1_score']:.3f} (¬±{overall['std_f1_score']:.3f})\")\n",
        "\n",
        "        # Detailed query results\n",
        "        print(f\"\\n DETAILED QUERY RESULTS:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"{'Query':<30} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
        "        # print(f\"{'Query':<30} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Retrieved':<10}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        for result in self.results['query_results']:\n",
        "            print(f\"{result['query'][:28]:<30} {result['precision']:<10.3f} {result['recall']:<10.3f} {result['f1_score']:<10.3f}\")\n",
        "            # print(f\"{result['query'][:28]:<30} {result['precision']:<10.3f} {result['recall']:<10.3f} {result['f1_score']:<10.3f} {result['retrieved_count']:<10}\")\n",
        "\n",
        "    def plot_evaluation_metrics(self):\n",
        "        \"\"\"Plot evaluation metrics\"\"\"\n",
        "        if not self.results:\n",
        "            print(\" No evaluation results available.\")\n",
        "            return\n",
        "\n",
        "        # Prepare data for plotting\n",
        "        queries = [f\"Q{i+1}\" for i in range(len(self.results['query_results']))]\n",
        "        precisions = [r['precision'] for r in self.results['query_results']]\n",
        "        recalls = [r['recall'] for r in self.results['query_results']]\n",
        "        f1_scores = [r['f1_score'] for r in self.results['query_results']]\n",
        "\n",
        "        # Create subplots\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Bar plot for individual query metrics\n",
        "        x = np.arange(len(queries))\n",
        "        width = 0.25\n",
        "\n",
        "        ax1.bar(x - width, precisions, width, label='Precision', alpha=0.8, color='skyblue')\n",
        "        ax1.bar(x, recalls, width, label='Recall', alpha=0.8, color='lightgreen')\n",
        "        ax1.bar(x + width, f1_scores, width, label='F1-Score', alpha=0.8, color='salmon')\n",
        "\n",
        "        ax1.set_xlabel('Queries')\n",
        "        ax1.set_ylabel('Scores')\n",
        "        ax1.set_title('Evaluation Metrics by Query')\n",
        "        ax1.set_xticks(x)\n",
        "        ax1.set_xticklabels(queries, rotation=45)\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Overall metrics comparison\n",
        "        overall = self.results['overall_metrics']\n",
        "        metrics = ['Precision', 'Recall', 'F1-Score']\n",
        "        values = [overall['mean_precision'], overall['mean_recall'], overall['mean_f1_score']]\n",
        "        errors = [overall['std_precision'], overall['std_recall'], overall['std_f1_score']]\n",
        "\n",
        "        bars = ax2.bar(metrics, values, yerr=errors, capsize=5, alpha=0.7,\n",
        "                      color=['skyblue', 'lightgreen', 'salmon'])\n",
        "        ax2.set_ylabel('Score')\n",
        "        ax2.set_title('Overall Evaluation Metrics')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, value in zip(bars, values):\n",
        "            height = bar.get_height()\n",
        "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                    f'{value:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "ntCu5Ft-WDNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Audio Quality Comparison Metrics\n",
        "class AudioQualityComparator:\n",
        "    def __init__(self):\n",
        "        self.metrics_history = []\n",
        "\n",
        "    def compare_audio_quality(self, original_path, processed_segments):\n",
        "        \"\"\"Compare original audio with processed segments\"\"\"\n",
        "        print(\" Comparing Audio Quality...\")\n",
        "\n",
        "        try:\n",
        "            # Load original audio\n",
        "            original_audio, original_sr = librosa.load(original_path, sr=16000)\n",
        "            original_duration = len(original_audio) / original_sr\n",
        "\n",
        "            # Calculate metrics for original audio\n",
        "            original_metrics = self._calculate_audio_metrics(original_audio, original_sr, \"Original\")\n",
        "\n",
        "            # Calculate metrics for processed segments\n",
        "            segment_metrics = []\n",
        "            for i, segment in enumerate(processed_segments):\n",
        "                segment_audio = segment['audio_segment']\n",
        "                segment_sr = 16000  # Assuming same sample rate\n",
        "\n",
        "                metrics = self._calculate_audio_metrics(segment_audio, segment_sr, f\"Segment_{i+1}\")\n",
        "                segment_metrics.append(metrics)\n",
        "\n",
        "            # Aggregate segment metrics\n",
        "            avg_segment_metrics = self._aggregate_segment_metrics(segment_metrics)\n",
        "\n",
        "            comparison_results = {\n",
        "                'original': original_metrics,\n",
        "                'segments_avg': avg_segment_metrics,\n",
        "                'segment_details': segment_metrics,\n",
        "                'original_duration': original_duration,\n",
        "                'total_segments_duration': sum([m['duration'] for m in segment_metrics])\n",
        "            }\n",
        "\n",
        "            self._display_comparison_report(comparison_results)\n",
        "            return comparison_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error in audio quality comparison: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _calculate_audio_metrics(self, audio, sr, label):\n",
        "        \"\"\"Calculate various audio quality metrics\"\"\"\n",
        "        duration = len(audio) / sr\n",
        "\n",
        "        # Basic audio properties\n",
        "        rms_energy = np.sqrt(np.mean(audio**2))\n",
        "        max_amplitude = np.max(np.abs(audio))\n",
        "\n",
        "        # Spectral features\n",
        "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr).mean()\n",
        "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr).mean()\n",
        "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr).mean()\n",
        "\n",
        "        # Zero crossing rate\n",
        "        zcr = librosa.feature.zero_crossing_rate(audio).mean()\n",
        "\n",
        "        # Signal-to-noise ratio (simplified)\n",
        "        noise_floor = np.std(audio[:min(1000, len(audio))])  # First 1000 samples as noise estimate\n",
        "        snr = 20 * np.log10(rms_energy / noise_floor) if noise_floor > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'label': label,\n",
        "            'duration': duration,\n",
        "            'rms_energy': rms_energy,\n",
        "            'max_amplitude': max_amplitude,\n",
        "            'spectral_centroid': spectral_centroid,\n",
        "            'spectral_bandwidth': spectral_bandwidth,\n",
        "            'spectral_rolloff': spectral_rolloff,\n",
        "            'zero_crossing_rate': zcr,\n",
        "            'snr_estimate': snr\n",
        "        }\n",
        "\n",
        "    def _aggregate_segment_metrics(self, segment_metrics):\n",
        "        \"\"\"Aggregate metrics across all segments\"\"\"\n",
        "        aggregated = {}\n",
        "        for key in segment_metrics[0].keys():\n",
        "            if key != 'label':\n",
        "                values = [m[key] for m in segment_metrics]\n",
        "                aggregated[key] = {\n",
        "                    'mean': np.mean(values),\n",
        "                    'std': np.std(values),\n",
        "                    'min': np.min(values),\n",
        "                    'max': np.max(values)\n",
        "                }\n",
        "        return aggregated\n",
        "\n",
        "    def _display_comparison_report(self, results):\n",
        "        \"\"\"Display comprehensive comparison report\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\" AUDIO QUALITY COMPARISON REPORT\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        orig = results['original']\n",
        "        avg = results['segments_avg']\n",
        "\n",
        "        print(f\"\\n  DURATION ANALYSIS:\")\n",
        "        print(f\"   Original Audio: {results['original_duration']:.2f}s\")\n",
        "        print(f\"   Total Segments: {results['total_segments_duration']:.2f}s\")\n",
        "        print(f\"   Coverage: {(results['total_segments_duration'] / results['original_duration'] * 100):.1f}%\")\n",
        "\n",
        "        print(f\"\\n AUDIO METRICS COMPARISON:\")\n",
        "        print(f\"{'Metric':<25} {'Original':<12} {'Segments Avg':<12} {'Difference':<12}\")\n",
        "        print(\"-\" * 65)\n",
        "\n",
        "        metrics_to_show = ['rms_energy', 'max_amplitude', 'spectral_centroid', 'zero_crossing_rate']\n",
        "        metric_names = {\n",
        "            'rms_energy': 'RMS Energy',\n",
        "            'max_amplitude': 'Max Amplitude',\n",
        "            'spectral_centroid': 'Spectral Centroid',\n",
        "            'zero_crossing_rate': 'Zero Crossing Rate'\n",
        "        }\n",
        "\n",
        "        for metric in metrics_to_show:\n",
        "            orig_val = orig[metric]\n",
        "            seg_val = avg[metric]['mean']\n",
        "            diff = seg_val - orig_val\n",
        "            diff_pct = (diff / orig_val * 100) if orig_val != 0 else 0\n",
        "\n",
        "            print(f\"{metric_names[metric]:<25} {orig_val:<12.4f} {seg_val:<12.4f} {diff_pct:>7.1f}%\")\n",
        "\n",
        "    def plot_audio_quality_comparison(self, comparison_results):\n",
        "        \"\"\"Plot audio quality comparison\"\"\"\n",
        "        if not comparison_results:\n",
        "            print(\" No comparison results available.\")\n",
        "            return\n",
        "\n",
        "        orig = comparison_results['original']\n",
        "        avg = comparison_results['segments_avg']\n",
        "\n",
        "        # Prepare data for plotting\n",
        "        metrics = ['RMS Energy', 'Max Amplitude', 'Spectral Centroid', 'Zero Crossing Rate']\n",
        "        original_vals = [\n",
        "            orig['rms_energy'],\n",
        "            orig['max_amplitude'],\n",
        "            orig['spectral_centroid'],\n",
        "            orig['zero_crossing_rate']\n",
        "        ]\n",
        "        segment_vals = [\n",
        "            avg['rms_energy']['mean'],\n",
        "            avg['max_amplitude']['mean'],\n",
        "            avg['spectral_centroid']['mean'],\n",
        "            avg['zero_crossing_rate']['mean']\n",
        "        ]\n",
        "        segment_stds = [\n",
        "            avg['rms_energy']['std'],\n",
        "            avg['max_amplitude']['std'],\n",
        "            avg['spectral_centroid']['std'],\n",
        "            avg['zero_crossing_rate']['std']\n",
        "        ]\n",
        "\n",
        "        # Create plot\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Bar plot comparison\n",
        "        x = np.arange(len(metrics))\n",
        "        width = 0.35\n",
        "\n",
        "        bars1 = ax1.bar(x - width/2, original_vals, width, label='Original Audio', alpha=0.8, color='blue')\n",
        "        bars2 = ax1.bar(x + width/2, segment_vals, width, label='Segments (Avg)', alpha=0.8, color='red', yerr=segment_stds, capsize=5)\n",
        "\n",
        "        ax1.set_xlabel('Audio Metrics')\n",
        "        ax1.set_ylabel('Values')\n",
        "        ax1.set_title('Audio Quality Metrics Comparison')\n",
        "        ax1.set_xticks(x)\n",
        "        ax1.set_xticklabels(metrics, rotation=45)\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Duration analysis\n",
        "        durations = [comparison_results['original_duration'], comparison_results['total_segments_duration']]\n",
        "        labels = ['Original Audio', 'All Segments']\n",
        "        colors = ['blue', 'red']\n",
        "\n",
        "        ax2.bar(labels, durations, color=colors, alpha=0.8)\n",
        "        ax2.set_ylabel('Duration (seconds)')\n",
        "        ax2.set_title('Audio Duration Comparison')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for i, v in enumerate(durations):\n",
        "            ax2.text(i, v + max(durations)*0.01, f'{v:.2f}s', ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "AxG2DJTAWG_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Comprehensive Evaluation\n",
        "def run_complete_evaluation(retrieval_system, audio_path):\n",
        "    \"\"\"Run complete evaluation pipeline\"\"\"\n",
        "    print(\" STARTING COMPREHENSIVE EVALUATION PIPELINE\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Initialize evaluators\n",
        "    retrieval_evaluator = AudioRetrievalEvaluator(retrieval_system)\n",
        "    quality_comparator = AudioQualityComparator()\n",
        "\n",
        "    # Test queries for evaluation\n",
        "    test_queries = [\n",
        "        \"data structures and algorithms\",\n",
        "        \"machine learning models\",\n",
        "        \"database management systems\",\n",
        "        \"computer networking protocols\",\n",
        "        \"software development process\",\n",
        "        \"artificial intelligence applications\",\n",
        "        \"computer security principles\",\n",
        "        \"web development frameworks\",\n",
        "        \"cloud computing services\",\n",
        "        \"operating system concepts\"\n",
        "    ]\n",
        "\n",
        "    # 1. Evaluate retrieval performance\n",
        "    print(\"\\n1.  EVALUATING RETRIEVAL PERFORMANCE...\")\n",
        "    retrieval_results = retrieval_evaluator.comprehensive_evaluation(test_queries)\n",
        "    retrieval_evaluator.generate_evaluation_report()\n",
        "    retrieval_evaluator.plot_evaluation_metrics()\n",
        "\n",
        "    # 2. Compare audio quality\n",
        "    print(\"\\n2.  COMPARING AUDIO QUALITY...\")\n",
        "    if hasattr(retrieval_system, 'segment_data'):\n",
        "        quality_results = quality_comparator.compare_audio_quality(\n",
        "            audio_path,\n",
        "            retrieval_system.segment_data['segments']\n",
        "        )\n",
        "        if quality_results:\n",
        "            quality_comparator.plot_audio_quality_comparison(quality_results)\n",
        "    else:\n",
        "        print(\" No segment data available for quality comparison\")\n",
        "\n",
        "# Run the evaluation\n",
        "evaluation_results = run_complete_evaluation(adaptive_system, audio_file)"
      ],
      "metadata": {
        "id": "yIdybB3iWLWR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}